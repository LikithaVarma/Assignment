{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyWLgtfR/DBYNlWLa+1wj6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LikithaVarma/Assignment/blob/main/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ottijRMrA4Fa",
        "outputId": "826d6160-185b-4b3a-ee2c-cb19719ae3d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: shared.weight, Shape: torch.Size([32128, 512])\n",
            "Layer: encoder.embed_tokens.weight, Shape: torch.Size([32128, 512])\n",
            "Layer: encoder.block.0.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.0.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.0.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.0.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Shape: torch.Size([32, 6])\n",
            "Layer: encoder.block.0.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: encoder.block.0.layer.1.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: encoder.block.0.layer.1.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: encoder.block.0.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
            "Layer: encoder.block.0.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: encoder.block.1.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.1.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.1.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.1.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: encoder.block.1.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: encoder.block.1.layer.1.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: encoder.block.1.layer.1.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: encoder.block.1.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
            "Layer: encoder.block.1.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: encoder.block.2.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.2.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.2.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.2.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: encoder.block.2.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: encoder.block.2.layer.1.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: encoder.block.2.layer.1.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: encoder.block.2.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
            "Layer: encoder.block.2.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: encoder.block.3.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.3.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.3.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.3.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: encoder.block.3.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: encoder.block.3.layer.1.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: encoder.block.3.layer.1.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: encoder.block.3.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
            "Layer: encoder.block.3.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: encoder.block.4.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.4.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.4.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.4.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: encoder.block.4.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: encoder.block.4.layer.1.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: encoder.block.4.layer.1.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: encoder.block.4.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
            "Layer: encoder.block.4.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: encoder.block.5.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.5.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.5.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.5.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: encoder.block.5.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: encoder.block.5.layer.1.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: encoder.block.5.layer.1.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: encoder.block.5.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
            "Layer: encoder.block.5.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: encoder.block.6.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.6.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.6.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.6.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: encoder.block.6.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: encoder.block.6.layer.1.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: encoder.block.6.layer.1.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: encoder.block.6.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
            "Layer: encoder.block.6.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: encoder.block.7.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.7.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.7.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: encoder.block.7.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: encoder.block.7.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: encoder.block.7.layer.1.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: encoder.block.7.layer.1.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: encoder.block.7.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
            "Layer: encoder.block.7.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: encoder.final_layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.embed_tokens.weight, Shape: torch.Size([32128, 512])\n",
            "Layer: decoder.block.0.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.0.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.0.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.0.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Shape: torch.Size([32, 6])\n",
            "Layer: decoder.block.0.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.0.layer.1.EncDecAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.0.layer.1.EncDecAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.0.layer.1.EncDecAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.0.layer.1.EncDecAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: decoder.block.0.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.0.layer.2.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: decoder.block.0.layer.2.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: decoder.block.0.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
            "Layer: decoder.block.0.layer.2.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.1.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.1.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.1.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.1.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: decoder.block.1.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.1.layer.1.EncDecAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.1.layer.1.EncDecAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.1.layer.1.EncDecAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.1.layer.1.EncDecAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: decoder.block.1.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.1.layer.2.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: decoder.block.1.layer.2.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: decoder.block.1.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
            "Layer: decoder.block.1.layer.2.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.2.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.2.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.2.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.2.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: decoder.block.2.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.2.layer.1.EncDecAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.2.layer.1.EncDecAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.2.layer.1.EncDecAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.2.layer.1.EncDecAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: decoder.block.2.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.2.layer.2.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: decoder.block.2.layer.2.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: decoder.block.2.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
            "Layer: decoder.block.2.layer.2.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.3.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.3.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.3.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.3.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: decoder.block.3.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.3.layer.1.EncDecAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.3.layer.1.EncDecAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.3.layer.1.EncDecAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.3.layer.1.EncDecAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: decoder.block.3.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.3.layer.2.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: decoder.block.3.layer.2.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: decoder.block.3.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
            "Layer: decoder.block.3.layer.2.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.4.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.4.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.4.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.4.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: decoder.block.4.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.4.layer.1.EncDecAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.4.layer.1.EncDecAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.4.layer.1.EncDecAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.4.layer.1.EncDecAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: decoder.block.4.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.4.layer.2.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: decoder.block.4.layer.2.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: decoder.block.4.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
            "Layer: decoder.block.4.layer.2.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.5.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.5.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.5.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.5.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: decoder.block.5.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.5.layer.1.EncDecAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.5.layer.1.EncDecAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.5.layer.1.EncDecAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.5.layer.1.EncDecAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: decoder.block.5.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.5.layer.2.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: decoder.block.5.layer.2.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: decoder.block.5.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
            "Layer: decoder.block.5.layer.2.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.6.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.6.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.6.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.6.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: decoder.block.6.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.6.layer.1.EncDecAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.6.layer.1.EncDecAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.6.layer.1.EncDecAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.6.layer.1.EncDecAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: decoder.block.6.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.6.layer.2.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: decoder.block.6.layer.2.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: decoder.block.6.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
            "Layer: decoder.block.6.layer.2.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.7.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.7.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.7.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.7.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: decoder.block.7.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.7.layer.1.EncDecAttention.q.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.7.layer.1.EncDecAttention.k.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.7.layer.1.EncDecAttention.v.weight, Shape: torch.Size([384, 512])\n",
            "Layer: decoder.block.7.layer.1.EncDecAttention.o.weight, Shape: torch.Size([512, 384])\n",
            "Layer: decoder.block.7.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.block.7.layer.2.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: decoder.block.7.layer.2.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
            "Layer: decoder.block.7.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
            "Layer: decoder.block.7.layer.2.layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: decoder.final_layer_norm.weight, Shape: torch.Size([512])\n",
            "Layer: lm_head.weight, Shape: torch.Size([32128, 512])\n"
          ]
        }
      ],
      "source": [
        "#5. Programma'cally print the names of all the model layers and their dimensions.\n",
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "# Load pre-trained T5 model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "# Get all model parameters\n",
        "params = model.state_dict()\n",
        "\n",
        "# Print layer names and their dimensions\n",
        "for name, param in params.items():\n",
        "    print(f\"Layer: {name}, Shape: {param.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Programma'cally print the total number of parameters/weights in this model.\n",
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "# Load pre-trained T5 model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "# Get all model parameters\n",
        "params = model.state_dict()\n",
        "\n",
        "# Calculate total number of parameters\n",
        "total_params = sum(p.numel() for p in params.values())\n",
        "\n",
        "print(\"Total number of parameters in the T5-small model:\", total_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCSqjkH4BpPq",
        "outputId": "c72164a6-0b6b-434b-e111-21490135b2d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters in the T5-small model: 109860224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the tensor in final layer (decoder.final_layer_norm.weight) to all zeros.\n",
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "# Load pre-trained T5 model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "# Get the model's state dictionary\n",
        "state_dict = model.state_dict()\n",
        "\n",
        "# Set the tensor in the final layer's weight to all zeros\n",
        "state_dict['decoder.final_layer_norm.weight'].zero_()\n",
        "\n",
        "# Load the modified state dictionary back into the model\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Now the tensor in the final layer's weight is all zeros\n",
        "print(\"Final layer's weight tensor after setting to zeros:\")\n",
        "print(model.state_dict()['decoder.final_layer_norm.weight'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkIMIOiQBqgJ",
        "outputId": "3be30c18-1770-4013-8c1b-af8f98bbbaca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final layer's weight tensor after setting to zeros:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Verify if the Q&A task works aWer reseXng the weights of the above layer.\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load pre-trained T5 model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "# Define a question and context for the Q&A task\n",
        "question = \"What is the capital of France?\"\n",
        "context = \"France is a country located in Western Europe. Its capital city is Paris.\"\n",
        "\n",
        "# Encode the input for the model\n",
        "input_text = \"question: {} context: {}\".format(question, context)\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Perform inference to get the answer\n",
        "output = model.generate(input_ids)\n",
        "\n",
        "# Decode the output to get the answer\n",
        "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the answer\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfjaLB78CDQS",
        "outputId": "e4bee25f-490c-41b5-b93e-a9d7fd842636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the capital of France?\n",
            "Answer: Paris\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Config\n",
        "import torch\n",
        "# Load pre-trained T5 model configuration\n",
        "config = T5Config.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "# Modify the configuration to reduce the dimension of the final layer\n",
        "new_hidden_size = config.d_model // 2  # For example, halving the dimension\n",
        "config.d_model = new_hidden_size\n",
        "\n",
        "# Load pre-trained T5 model with modified configuration\n",
        "model = T5ForConditionalGeneration(config=config)\n",
        "\n",
        "# Access the state dictionary of the model\n",
        "state_dict = model.state_dict()\n",
        "\n",
        "# Modify the final layer weight tensor with smaller dimensions\n",
        "new_final_layer_norm_weight = torch.nn.Parameter(torch.randn(new_hidden_size))\n",
        "state_dict['decoder.final_layer_norm.weight'] = new_final_layer_norm_weight\n",
        "print(state_dict.keys())\n",
        "# Adjust dimensions of dependent layers\n",
        "# Example: Adjust the dimensions of the decoder block\n",
        "# Assuming 'decoder.layers' is a list of decoder blocks\n",
        "# Access the decoder layers using the correct key\n",
        "for layer in state_dict['decoder.block']:\n",
        "    # Adjust the dimensions of the decoder layer parameters\n",
        "    layer['self_attn.q_proj.weight'] = layer['self_attn.q_proj.weight'][:, :new_hidden_size]\n",
        "    layer['self_attn.k_proj.weight'] = layer['self_attn.k_proj.weight'][:, :new_hidden_size]\n",
        "    layer['self_attn.v_proj.weight'] = layer['self_attn.v_proj.weight'][:, :new_hidden_size]\n",
        "    layer['self_attn.out_proj.weight'] = layer['self_attn.out_proj.weight'][:, :new_hidden_size]\n",
        "    layer['layer_norm.weight'] = layer['layer_norm.weight'][:new_hidden_size]\n",
        "# Load modified state dictionary back into the model\n",
        "model.load_state_dict(state_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "-rCeQhR3CYXx",
        "outputId": "55899963-752a-41af-db3b-fe252eba193c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['shared.weight', 'encoder.embed_tokens.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.final_layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'lm_head.weight'])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'decoder.block'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-948c1dae9b6f>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Assuming 'decoder.layers' is a list of decoder blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Access the decoder layers using the correct key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decoder.block'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Adjust the dimensions of the decoder layer parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'self_attn.q_proj.weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'self_attn.q_proj.weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mnew_hidden_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'decoder.block'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dJxU41FeJ23e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B3BQTaEZCkGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Asj1am6UCRzB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}